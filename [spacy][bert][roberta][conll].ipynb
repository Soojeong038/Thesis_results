{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqN-itbgdu9P",
        "outputId": "6bd4a664-118d-4087-dedc-77615c4555d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.10.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (8.1.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.10.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.8-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy[transformers]) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.9.24)\n",
            "Collecting transformers<4.22.0,>=3.4.0\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.13.0+cu116)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.0.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 spacy-alignments-0.8.6 spacy-transformers-1.1.8 tokenizers-0.12.1 transformers-4.21.3\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy[transformers]\n",
        "import spacy_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_e-ITW15tBE"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCtihnGed5yz",
        "outputId": "7009627b-a518-4b93-9df3-f50c4624a2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output_roberta_conll\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output_roberta_conll\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-12-15 08:18:14,619] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2022-12-15 08:18:14,629] [INFO] Pipeline: ['transformer', 'ner']\n",
            "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
            "[2022-12-15 08:18:14,633] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2022-12-15 08:18:14,634] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Downloading tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 43.0kB/s]\n",
            "Downloading config.json: 100% 433/433 [00:00<00:00, 611kB/s]\n",
            "Downloading vocab.txt: 100% 249k/249k [00:00<00:00, 2.26MB/s]\n",
            "Downloading tokenizer.json: 100% 474k/474k [00:00<00:00, 3.34MB/s]\n",
            "Downloading pytorch_model.bin: 100% 419M/419M [00:04<00:00, 89.2MB/s]\n",
            "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-12-15 08:18:44,003] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving results to saveoutput.txt\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        1313.50    248.15    3.51    2.01   13.68    0.04\n",
            "  0      50       50670.83  22910.68    6.19   19.18    3.69    0.06\n",
            "  0     100        5679.23   3401.36   72.00   77.00   67.61    0.72\n",
            "  0     150         910.02   1589.09   80.06   83.12   77.22    0.80\n",
            "  1     200         662.44   1223.10   81.84   81.51   82.19    0.82\n",
            "  1     250         501.64    986.70   84.31   85.76   82.92    0.84\n",
            "  1     300         506.40   1081.42   84.12   85.75   82.56    0.84\n",
            "  2     350         384.96    787.90   83.95   87.12   81.00    0.84\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output_roberta_conll/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config_bert.cfg --gpu-id 0 --paths.train conll_train_spacy.spacy --paths.dev conll_testa_spacy.spacy --output ./output_bert_conll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX2yTNG3uJ_u",
        "outputId": "000c5b6a-b89f-42fd-eb3e-5b3c62bd4db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   82.07 \n",
            "NER R   78.52 \n",
            "NER F   80.25 \n",
            "SPEED   4198  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "LOC    78.17   83.79   80.88\n",
            "ORG    80.81   59.46   68.51\n",
            "PER    90.59   93.37   91.96\n",
            "MISC   73.05   66.00   69.35\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy evaluate ./output_bert_conll/model-best conll_testb_spacy.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8M2I_xd4w_B"
      },
      "source": [
        "Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUxWj39l438O",
        "outputId": "28ffe737-6f1c-43ce-de74-4d74fd1b2c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output_roberta_conll_wls\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output_roberta_conll_wls\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-12-15 08:29:56,738] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2022-12-15 08:29:56,748] [INFO] Pipeline: ['transformer', 'ner']\n",
            "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
            "[2022-12-15 08:29:56,751] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2022-12-15 08:29:56,753] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Downloading config.json: 100% 615/615 [00:00<00:00, 613kB/s]\n",
            "Downloading sentencepiece.bpe.model: 100% 4.83M/4.83M [00:00<00:00, 21.1MB/s]\n",
            "Downloading tokenizer.json: 100% 8.68M/8.68M [00:00<00:00, 29.2MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.04G/1.04G [00:14<00:00, 76.9MB/s]\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-12-15 08:30:24,511] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving results to saveoutput_roberta.txt\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 1200103424 bytes == 0x16b43c000 @  0x7f531d8c2615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f52a52bd5fe 0x7f5269df2c85 0x7f5269ded1e7 0x7f5269df4309 0x7f52a52d00bb 0x7f52a4ed06af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x4997a2\n",
            "tcmalloc: large alloc 1500135424 bytes == 0x104428000 @  0x7f531d8c2615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f52a52bd5fe 0x7f5269df2c85 0x7f5269ded1e7 0x7f5269df4309 0x7f52a52d00bb 0x7f52a4ed06af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x4997a2\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x7f4f8d688000 @  0x7f531d8c22a4 0x7f53122003a2 0x7f5312201cdf 0x7f53121fe675 0x7f53121fee2e 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4997c7 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91\n",
            "tcmalloc: large alloc 2268823552 bytes == 0x7f4f062d0000 @  0x7f531d8c22a4 0x7f53122003a2 0x7f531220223a 0x7f531220223a 0x7f5312201cdf 0x7f53121fe675 0x7f53121fee2e 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4997c7 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x55d078\n",
            "  0       0         222.43    325.46    3.69    2.01   21.71    0.04\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x7f4f062d0000 @  0x7f531d8c22a4 0x7f53122003a2 0x7f5312201cdf 0x7f53121fe675 0x7f53121fee2e 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4997c7 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91\n",
            "tcmalloc: large alloc 2268823552 bytes == 0x7f4f062d0000 @  0x7f531d8c22a4 0x7f53122003a2 0x7f531220223a 0x7f531220223a 0x7f5312201cdf 0x7f53121fe675 0x7f53121fee2e 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4997c7 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x55d078\n",
            "  0      50       20297.93  28348.36   11.59   23.10    7.74    0.12\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x7f4f062d0000 @  0x7f531d8c22a4 0x7f53122003a2 0x7f5312201cdf 0x7f53121fe675 0x7f53121fee2e 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4997c7 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x5d8868 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91\n",
            "  0     100        7386.90   3720.78   52.96   62.37   46.02    0.53\n",
            "  0     150        1851.04   2166.71   73.45   77.86   69.52    0.73\n",
            "  1     200         762.79   1489.42   80.72   81.36   80.10    0.81\n",
            "  1     250         537.54   1243.94   83.04   85.92   80.35    0.83\n",
            "  1     300         735.62   1413.82   83.42   84.09   82.77    0.83\n",
            "  2     350         505.27   1094.64   82.86   85.91   80.02    0.83\n",
            "  2     400         598.30   1105.97   82.13   83.91   80.41    0.82\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output_roberta_conll_wls/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config_roberta.cfg --gpu-id 0 --paths.train conll_train_spacy.spacy --paths.dev conll_testa_spacy.spacy --output ./output_roberta_conll_wls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-3PZ6cPGYKY",
        "outputId": "abfdb4c0-fbc5-4e39-823d-3e5fd7939e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "tcmalloc: large alloc 1134411776 bytes == 0x71a68000 @  0x7fc6ba7be1e7 0x4d30a0 0x5dede2 0x61033f 0x5aab9b 0x47c416 0x6170f1 0x4f7916 0x4997a2 0x55d078 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997c7 0x4fe253 0x49abe4 0x55d078 0x5d8941 0x5d8416 0x55f797 0x55d078 0x5d8941 0x4997c7 0x55d078\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   80.43 \n",
            "NER R   79.79 \n",
            "NER F   80.11 \n",
            "SPEED   3829  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "LOC    82.03   80.68   81.35\n",
            "ORG    74.29   67.36   70.65\n",
            "PER    90.45   91.84   91.14\n",
            "MISC   67.05   71.36   69.14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy evaluate ./output_roberta_conll_wls/model-best conll_testb_spacy.spacy --gpu-id 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
