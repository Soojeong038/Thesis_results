{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqN-itbgdu9P",
        "outputId": "c520a1f6-5281-43f5-93ae-985147855ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (4.64.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (8.1.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (1.10.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.0.11)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy[transformers]) (3.0.8)\n",
            "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.9-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy[transformers]) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (4.0.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.13.1+cu116)\n",
            "Collecting transformers<4.26.0,>=3.4.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.9.0)\n",
            "Installing collected packages: tokenizers, spacy-alignments, huggingface-hub, transformers, spacy-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 spacy-alignments-0.9.0 spacy-transformers-1.1.9 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy[transformers]\n",
        "import spacy_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8M2I_xd4w_B"
      },
      "source": [
        "Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUxWj39l438O",
        "outputId": "0a5df18a-fcc4-43d7-811f-b9c23b5bb697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output_roberta_conll_wls\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output_roberta_conll_wls\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-01-24 12:36:33,574] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2023-01-24 12:36:33,584] [INFO] Pipeline: ['transformer', 'ner']\n",
            "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
            "[2023-01-24 12:36:33,587] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2023-01-24 12:36:33,588] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Downloading: 100% 615/615 [00:00<00:00, 709kB/s]\n",
            "Downloading: 100% 5.07M/5.07M [00:00<00:00, 7.32MB/s]\n",
            "Downloading: 100% 9.10M/9.10M [00:00<00:00, 10.5MB/s]\n",
            "Downloading: 100% 1.12G/1.12G [00:10<00:00, 105MB/s] \n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-01-24 12:37:14,252] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving results to saveoutput_roberta_all.txt\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 1200103424 bytes == 0xa9f0c000 @  0x7f711fe99680 0x7f711feb9da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f70aad84795 0x7f70aaedceeb 0x7f7069b9dc45 0x7f7069b97728 0x7f7069b9f2c9 0x7f70aaef09ca 0x7f70aaad7f00 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x56bbe1\n",
            "tcmalloc: large alloc 1500135424 bytes == 0xf208e000 @  0x7f711fe99680 0x7f711feb9da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f70aad84795 0x7f70aaedceeb 0x7f7069b9dc45 0x7f7069b97728 0x7f7069b9f2c9 0x7f70aaef09ca 0x7f70aaad7f00 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x56bbe1\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x14b732000 @  0x7f711fe99680 0x7f711feb9bdd 0x7f71170643a2 0x7f7117065cdf 0x7f7117062675 0x7f7117062e2e 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x570b82 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0\n",
            "tcmalloc: large alloc 2268823552 bytes == 0xed8e8000 @  0x7f711fe99680 0x7f711feb9bdd 0x7f71170643a2 0x7f711706623a 0x7f711706623a 0x7f7117065cdf 0x7f7117062675 0x7f7117062e2e 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x570b82 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6\n",
            "  0       0         206.44    417.27    2.08    1.13   12.22    0.02\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x14b5fe000 @  0x7f711fe99680 0x7f711feb9bdd 0x7f71170643a2 0x7f7117065cdf 0x7f7117062675 0x7f7117062e2e 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x570b82 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0\n",
            "tcmalloc: large alloc 2268823552 bytes == 0xed7b4000 @  0x7f711fe99680 0x7f711feb9bdd 0x7f71170643a2 0x7f711706623a 0x7f711706623a 0x7f7117065cdf 0x7f7117062675 0x7f7117062e2e 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x570b82 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6\n",
            "  0     100       17615.51  35963.57   56.62   65.01   50.15    0.57\n",
            "tcmalloc: large alloc 2224521216 bytes == 0x14c9e6000 @  0x7f711fe99680 0x7f711feb9bdd 0x7f71170643a2 0x7f7117065cdf 0x7f7117062675 0x7f7117062e2e 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x5f5ee6 0x570b82 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0\n",
            "  1     200        3326.51   4149.43   81.08   82.94   79.31    0.81\n",
            "  1     300        1118.43   2632.95   83.80   86.09   81.62    0.84\n",
            "  2     400         798.47   1897.93   84.64   85.92   83.40    0.85\n",
            "  2     500         709.30   1665.10   84.64   85.92   83.40    0.85\n",
            "  3     600         712.19   1686.22   84.64   85.92   83.40    0.85\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output_roberta_conll_wls/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config_roberta.cfg --gpu-id 0 --paths.train conll_train_spacy.spacy --paths.dev conll_testa_spacy.spacy --output ./output_roberta_conll_wls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate ./output_roberta_conll_wls/model-best conll_testb_spacy.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP82ZalI1own",
        "outputId": "e749a37e-ae0b-4dc2-cfb5-9005c303f00a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "tcmalloc: large alloc 1134411776 bytes == 0x711c2000 @  0x7f5e45216680 0x7f5e45237824 0x5f97c1 0x649901 0x5c43c6 0x4f327e 0x64e618 0x505163 0x56bbe1 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x569d8a 0x5f60c3 0x56bab6 0x5f5ee6 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x569d8a 0x5f60c3 0x5f52b2 0x56d2bc 0x569d8a 0x5f60c3\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   84.93 \n",
            "NER R   79.12 \n",
            "NER F   81.92 \n",
            "SPEED   4852  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "MISC   78.35   68.95   73.35\n",
            "ORG    84.23   74.29   78.95\n",
            "LOC    80.85   82.98   81.90\n",
            "PER    93.75   87.41   90.47\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}